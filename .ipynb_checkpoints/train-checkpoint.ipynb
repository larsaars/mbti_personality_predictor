{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "indonesian-following",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\larsl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "classified-fountain",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "      type                                              posts\n0     INFJ  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...\n1     ENTP  'I'm finding the lack of me in these posts ver...\n2     INTP  'Good one  _____   https://www.youtube.com/wat...\n3     INTJ  'Dear INTP,   I enjoyed our conversation the o...\n4     ENTJ  'You're fired.|||That's another silly misconce...\n...    ...                                                ...\n8670  ISFP  'https://www.youtube.com/watch?v=t8edHB_h908||...\n8671  ENFP  'So...if this thread already exists someplace ...\n8672  INTP  'So many questions when i do these things.  I ...\n8673  INFP  'I am very conflicted right now when it comes ...\n8674  INFP  'It has been too long since I have been on per...\n\n[8675 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>type</th>\n      <th>posts</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>INFJ</td>\n      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw|||...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ENTP</td>\n      <td>'I'm finding the lack of me in these posts ver...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>INTP</td>\n      <td>'Good one  _____   https://www.youtube.com/wat...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>INTJ</td>\n      <td>'Dear INTP,   I enjoyed our conversation the o...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ENTJ</td>\n      <td>'You're fired.|||That's another silly misconce...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>8670</th>\n      <td>ISFP</td>\n      <td>'https://www.youtube.com/watch?v=t8edHB_h908||...</td>\n    </tr>\n    <tr>\n      <th>8671</th>\n      <td>ENFP</td>\n      <td>'So...if this thread already exists someplace ...</td>\n    </tr>\n    <tr>\n      <th>8672</th>\n      <td>INTP</td>\n      <td>'So many questions when i do these things.  I ...</td>\n    </tr>\n    <tr>\n      <th>8673</th>\n      <td>INFP</td>\n      <td>'I am very conflicted right now when it comes ...</td>\n    </tr>\n    <tr>\n      <th>8674</th>\n      <td>INFP</td>\n      <td>'It has been too long since I have been on per...</td>\n    </tr>\n  </tbody>\n</table>\n<p>8675 rows Ã— 2 columns</p>\n</div>"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('mbti.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "   type                                              posts  IE  NS  TF  JP\n0  INFJ  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...   1   1   0   1\n1  ENTP  'I'm finding the lack of me in these posts ver...   0   1   1   0\n2  INTP  'Good one  _____   https://www.youtube.com/wat...   1   1   1   0\n3  INTJ  'Dear INTP,   I enjoyed our conversation the o...   1   1   1   1\n4  ENTJ  'You're fired.|||That's another silly misconce...   0   1   1   1\n5  INTJ  '18/37 @.@|||Science  is not perfect. No scien...   1   1   1   1\n6  INFJ  'No, I can't draw on my own nails (haha). Thos...   1   1   0   1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>type</th>\n      <th>posts</th>\n      <th>IE</th>\n      <th>NS</th>\n      <th>TF</th>\n      <th>JP</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>INFJ</td>\n      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw|||...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ENTP</td>\n      <td>'I'm finding the lack of me in these posts ver...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>INTP</td>\n      <td>'Good one  _____   https://www.youtube.com/wat...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>INTJ</td>\n      <td>'Dear INTP,   I enjoyed our conversation the o...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ENTJ</td>\n      <td>'You're fired.|||That's another silly misconce...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>INTJ</td>\n      <td>'18/37 @.@|||Science  is not perfect. No scien...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>INFJ</td>\n      <td>'No, I can't draw on my own nails (haha). Thos...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_types(row):\n",
    "    t = row['type']\n",
    "\n",
    "    I, N, T, J = 0, 0, 0, 0\n",
    "\n",
    "    if t[0] == 'I':\n",
    "        I = 1\n",
    "    elif t[0] == 'E':\n",
    "        I = 0\n",
    "    else:\n",
    "        print('I-E not found')\n",
    "\n",
    "    if t[1] == 'N':\n",
    "        N = 1\n",
    "    elif t[1] == 'S':\n",
    "        N = 0\n",
    "    else:\n",
    "        print('N-S not found')\n",
    "\n",
    "    if t[2] == 'T':\n",
    "        T = 1\n",
    "    elif t[2] == 'F':\n",
    "        T = 0\n",
    "    else:\n",
    "        print('T-F not found')\n",
    "\n",
    "    if t[3] == 'J':\n",
    "        J = 1\n",
    "    elif t[3] == 'P':\n",
    "        J = 0\n",
    "    else:\n",
    "        print('J-P not found')\n",
    "    return pd.Series({'IE': I, 'NS': N, 'TF': T, 'JP': J})\n",
    "\n",
    "\n",
    "df = df.join(df.apply(lambda row: get_types(row), axis=1))\n",
    "df.head(7)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "df.to_csv('preprocessed.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "lemmatiser = WordNetLemmatizer()\n",
    "\n",
    "# Remove the stop words for speed\n",
    "useless_words = stopwords.words(\"english\")\n",
    "\n",
    "# Remove these from the posts\n",
    "unique_type_list = ['INFJ', 'ENTP', 'INTP', 'INTJ', 'ENTJ', 'ENFJ', 'INFP', 'ENFP',\n",
    "                    'ISFP', 'ISTP', 'ISFJ', 'ISTJ', 'ESTP', 'ESFP', 'ESTJ', 'ESFJ']\n",
    "unique_type_list = [x.lower() for x in unique_type_list]\n",
    "\n",
    "# Or we can use Label Encoding (as above) of this unique personality type indicator list\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# unique_type_list = ['INFJ', 'ENTP', 'INTP', 'INTJ', 'ENTJ', 'ENFJ', 'INFP', 'ENFP',\n",
    "#        'ISFP', 'ISTP', 'ISFJ', 'ISTJ', 'ESTP', 'ESFP', 'ESTJ', 'ESFJ']\n",
    "# lab_encoder = LabelEncoder().fit(unique_type_list)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binarize MBTI list: \n",
      "[[0 0 0 0]\n",
      " [1 0 1 1]\n",
      " [0 0 1 1]\n",
      " ...\n",
      " [0 0 1 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# Splitting the MBTI personality into 4 letters and binarizing it\n",
    "\n",
    "b_Pers = {'I': 0, 'E': 1, 'N': 0, 'S': 1, 'F': 0, 'T': 1, 'J': 0, 'P': 1}\n",
    "b_Pers_list = [{0: 'I', 1: 'E'}, {0: 'N', 1: 'S'}, {0: 'F', 1: 'T'}, {0: 'J', 1: 'P'}]\n",
    "\n",
    "\n",
    "def translate_personality(personality):\n",
    "    # transform mbti to binary vector\n",
    "    return [b_Pers[l] for l in personality]\n",
    "\n",
    "\n",
    "#To show result output for personality prediction\n",
    "def translate_back(personality):\n",
    "    # transform binary vector to mbti personality\n",
    "    s = \"\"\n",
    "    for i, l in enumerate(personality):\n",
    "        s += b_Pers_list[i][l]\n",
    "    return s\n",
    "\n",
    "\n",
    "list_personality_bin = np.array([translate_personality(p) for p in df.type])\n",
    "print(\"Binarize MBTI list: \\n%s\" % list_personality_bin)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "   type                                              posts  IE  NS  TF  JP\n0  INFJ  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...   1   1   0   1\n1  ENTP  'I'm finding the lack of me in these posts ver...   0   1   1   0\n2  INTP  'Good one  _____   https://www.youtube.com/wat...   1   1   1   0\n3  INTJ  'Dear INTP,   I enjoyed our conversation the o...   1   1   1   1\n4  ENTJ  'You're fired.|||That's another silly misconce...   0   1   1   1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>type</th>\n      <th>posts</th>\n      <th>IE</th>\n      <th>NS</th>\n      <th>TF</th>\n      <th>JP</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>INFJ</td>\n      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw|||...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ENTP</td>\n      <td>'I'm finding the lack of me in these posts ver...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>INTP</td>\n      <td>'Good one  _____   https://www.youtube.com/wat...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>INTJ</td>\n      <td>'Dear INTP,   I enjoyed our conversation the o...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ENTJ</td>\n      <td>'You're fired.|||That's another silly misconce...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example :\n",
      "\n",
      "Post before preprocessing:\n",
      "\n",
      " 'http://www.youtube.com/watch?v=qsXHcwe3krw|||http://41.media.tumblr.com/tumblr_lfouy03PMA1qa1rooo1_500.jpg|||enfp and intj moments  https://www.youtube.com/watch?v=iz7lE1g4XM4  sportscenter not top ten plays  https://www.youtube.com/watch?v=uCdfze1etec  pranks|||What has been the most life-changing experience in your life?|||http://www.youtube.com/watch?v=vXZeYwwRDw8   http://www.youtube.com/watch?v=u8ejam5DP3E  On repeat for most of today.|||May the PerC Experience immerse you.|||The last thing my INFJ friend posted on his facebook before committing suicide the next day. Rest in peace~   http://vimeo.com/22842206|||Hello ENFJ7. Sorry to hear of your distress. It's only natural for a relationship to not be perfection all the time in every moment of existence. Try to figure the hard times as times of growth, as...|||84389  84390  http://wallpaperpassion.com/upload/23700/friendship-boy-and-girl-wallpaper.jpg  http://assets.dornob.com/wp-content/uploads/2010/04/round-home-design.jpg ...|||Welcome and stuff.|||http://playeressence.com/wp-content/uploads/2013/08/RED-red-the-pokemon-master-32560474-450-338.jpg  Game. Set. Match.|||Prozac, wellbrutin, at least thirty minutes of moving your legs (and I don't mean moving them while sitting in your same desk chair), weed in moderation (maybe try edibles as a healthier alternative...|||Basically come up with three items you've determined that each type (or whichever types you want to do) would more than likely use, given each types' cognitive functions and whatnot, when left by...|||All things in moderation.  Sims is indeed a video game, and a good one at that. Note: a good one at that is somewhat subjective in that I am not completely promoting the death of any given Sim...|||Dear ENFP:  What were your favorite video games growing up and what are your now, current favorite video games? :cool:|||https://www.youtube.com/watch?v=QyPqT8umzmY|||It appears to be too late. :sad:|||There's someone out there for everyone.|||Wait... I thought confidence was a good thing.|||I just cherish the time of solitude b/c i revel within my inner world more whereas most other time i'd be workin... just enjoy the me time while you can. Don't worry, people will always be around to...|||Yo entp ladies... if you're into a complimentary personality,well, hey.|||... when your main social outlet is xbox live conversations and even then you verbally fatigue quickly.|||http://www.youtube.com/watch?v=gDhy7rdfm14  I really dig the part from 1:46 to 2:50|||http://www.youtube.com/watch?v=msqXffgh7b8|||Banned because this thread requires it of me.|||Get high in backyard, roast and eat marshmellows in backyard while conversing over something intellectual, followed by massages and kisses.|||http://www.youtube.com/watch?v=Mw7eoU3BMbE|||http://www.youtube.com/watch?v=4V2uYORhQOk|||http://www.youtube.com/watch?v=SlVmgFQQ0TI|||Banned for too many b's in that sentence. How could you! Think of the B!|||Banned for watching movies in the corner with the dunces.|||Banned because Health class clearly taught you nothing about peer pressure.|||Banned for a whole host of reasons!|||http://www.youtube.com/watch?v=IRcrv41hgz4|||1) Two baby deer on left and right munching on a beetle in the middle.  2) Using their own blood, two cavemen diary today's latest happenings on their designated cave diary wall.  3) I see it as...|||a pokemon world  an infj society  everyone becomes an optimist|||49142|||http://www.youtube.com/watch?v=ZRCEq_JFeFM|||http://discovermagazine.com/2012/jul-aug/20-things-you-didnt-know-about-deserts/desert.jpg|||http://oyster.ignimgs.com/mediawiki/apis.ign.com/pokemon-silver-version/d/dd/Ditto.gif|||http://www.serebii.net/potw-dp/Scizor.jpg|||Not all artists are artists because they draw. It's the idea that counts in forming something of your own... like a signature.|||Welcome to the robot ranks, person who downed my self-esteem cuz I'm not an avid signature artist like herself. :proud:|||Banned for taking all the room under my bed. Ya gotta learn to share with the roaches.|||http://www.youtube.com/watch?v=w8IgImn57aQ|||Banned for being too much of a thundering, grumbling kind of storm... yep.|||Ahh... old high school music I haven't heard in ages.   http://www.youtube.com/watch?v=dcCRUPCdB1w|||I failed a public speaking class a few years ago and I've sort of learned what I could do better were I to be in that position again. A big part of my failure was just overloading myself with too...|||I like this person's mentality. He's a confirmed INTJ by the way. http://www.youtube.com/watch?v=hGKLI-GEc6M|||Move to the Denver area and start a new life for myself.'\n",
      "\n",
      "Post after preprocessing:\n",
      "\n",
      "    moment sportscenter top ten play prank life changing experience life repeat today may perc experience immerse last thing  friend posted facebook committing suicide next day rest peace hello  sorry hear distress natural relationship perfection time every moment existence try figure hard time time growth welcome stuff game set match prozac wellbrutin least thirty minute moving leg mean moving sitting desk chair weed moderation maybe try edible healthier alternative basically come three item determined type whichever type want would likely use given type cognitive function whatnot left thing moderation sims indeed video game good one note good one somewhat subjective completely promoting death given sim dear  favorite video game growing current favorite video game cool appears late sad someone everyone wait thought confidence good thing cherish time solitude b c revel within inner world whereas time workin enjoy time worry people always around yo  lady complimentary personality well hey main social outlet xbox live conversation even verbally fatigue quickly really dig part banned thread requires get high backyard roast eat marshmellows backyard conversing something intellectual followed massage kiss banned many b sentence could think b banned watching movie corner dunce banned health class clearly taught nothing peer pressure banned whole host reason two baby deer left right munching beetle middle using blood two caveman diary today latest happening designated cave diary wall see pokemon world  society everyone becomes optimist artist artist draw idea count forming something like signature welcome robot rank person downed self esteem cuz avid signature artist like proud banned taking room bed ya gotta learn share roach banned much thundering grumbling kind storm yep ahh old high school music heard age failed public speaking class year ago sort learned could better position big part failure overloading like person mentality confirmed  way move denver area start new life \n",
      "\n",
      "MBTI before preprocessing:\n",
      "\n",
      " INFJ\n",
      "\n",
      "MBTI after preprocessing:\n",
      "\n",
      " [0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "def pre_process_text(data, remove_stop_words=True, remove_mbti_profiles=True):\n",
    "    list_personality = []\n",
    "    list_posts = []\n",
    "\n",
    "    for row in data.iterrows():\n",
    "        # check code working\n",
    "        # i+=1\n",
    "        # if (i % 500 == 0 or i == 1 or i == len_data):\n",
    "        #     print(\"%s of %s rows\" % (i, len_data))\n",
    "\n",
    "        #Remove and clean comments\n",
    "        posts = row[1].posts\n",
    "\n",
    "        #Remove url links\n",
    "        temp = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' ', posts)\n",
    "\n",
    "        #Remove Non-words - keep only words\n",
    "        temp = re.sub(\"[^a-zA-Z]\", \" \", temp)\n",
    "\n",
    "        # Remove spaces > 1\n",
    "        temp = re.sub(' +', ' ', temp).lower()\n",
    "\n",
    "        #Remove multiple letter repeating words\n",
    "        temp = re.sub(r'([a-z])\\1{2,}[\\s|\\w]*', '', temp)\n",
    "\n",
    "        #Remove stop words\n",
    "        if remove_stop_words:\n",
    "            temp = \" \".join([lemmatiser.lemmatize(w) for w in temp.split(' ') if w not in useless_words])\n",
    "        else:\n",
    "            temp = \" \".join([lemmatiser.lemmatize(w) for w in temp.split(' ')])\n",
    "\n",
    "        #Remove MBTI personality words from posts\n",
    "        if remove_mbti_profiles:\n",
    "            for t in unique_type_list:\n",
    "                temp = temp.replace(t, '')\n",
    "\n",
    "        # transform mbti to binary vector\n",
    "        type_labelized = translate_personality(row[1].type)  #or use lab_encoder.transform([row[1].type])[0]\n",
    "        list_personality.append(type_labelized)\n",
    "        # the cleaned data temp is passed here\n",
    "        list_posts.append(temp)\n",
    "\n",
    "    # returns the result\n",
    "    list_posts = np.array(list_posts)\n",
    "    list_personality = np.array(list_personality)\n",
    "    return list_posts, list_personality\n",
    "\n",
    "\n",
    "list_posts, list_personality = pre_process_text(df, remove_stop_words=True, remove_mbti_profiles=True)\n",
    "\n",
    "print(\"Example :\")\n",
    "print(\"\\nPost before preprocessing:\\n\\n\", df.posts[0])\n",
    "print(\"\\nPost after preprocessing:\\n\\n\", list_posts[0])\n",
    "print(\"\\nMBTI before preprocessing:\\n\\n\", df.type[0])\n",
    "print(\"\\nMBTI after preprocessing:\\n\\n\", list_personality[0])\n",
    "\n",
    "df.to_csv('preprocessed.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0]\n",
      " [1 0 1 1]\n",
      " [0 0 1 1]\n",
      " ...\n",
      " [0 0 1 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]]\n"
     ]
    },
    {
     "data": {
      "text/plain": "(8675, 4)"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(list_personality)\n",
    "list_personality.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CountVectorizer :\n",
      "10 feature names can be seen below\n",
      "[(0, 'ability'), (1, 'able'), (2, 'absolutely'), (3, 'across'), (4, 'act'), (5, 'action'), (6, 'actually'), (7, 'add'), (8, 'advice'), (9, 'afraid')]\n",
      "\n",
      "Using Tf-idf :\n",
      "Now the dataset size is as below\n",
      "(8675, 595)\n"
     ]
    }
   ],
   "source": [
    "# Vectorizing the database posts to a matrix of token counts for the model\n",
    "cntizer = CountVectorizer(analyzer=\"word\",\n",
    "                          max_features=1000,\n",
    "                          max_df=0.7,\n",
    "                          min_df=0.1)\n",
    "# the feature should be made of word n-gram\n",
    "# Learn the vocabulary dictionary and return term-document matrix\n",
    "print(\"Using CountVectorizer :\")\n",
    "X_cnt = cntizer.fit_transform(list_posts)\n",
    "\n",
    "#The enumerate object yields pairs containing a count and a value (useful for obtaining an indexed list)\n",
    "feature_names = list(enumerate(cntizer.get_feature_names()))\n",
    "print(\"10 feature names can be seen below\")\n",
    "print(feature_names[0:10])\n",
    "\n",
    "# For the Standardization or Feature Scaling Stage :-\n",
    "# Transform the count matrix to a normalized tf or tf-idf representation\n",
    "tfizer = TfidfTransformer()\n",
    "\n",
    "# Learn the idf vector (fit) and transform a count matrix to a tf-idf representation\n",
    "print(\"\\nUsing Tf-idf :\")\n",
    "\n",
    "print(\"Now the dataset size is as below\")\n",
    "X_tfidf = tfizer.fit_transform(X_cnt).toarray()\n",
    "print(X_tfidf.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "['feel',\n 'type',\n 'well',\n 'say',\n 'way',\n 'friend',\n 'want',\n 'love',\n 'good',\n 'something']"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#counting top 10 words\n",
    "reverse_dic = {}\n",
    "for key in cntizer.vocabulary_:\n",
    "    reverse_dic[cntizer.vocabulary_[key]] = key\n",
    "top_10 = np.asarray(np.argsort(np.sum(X_cnt, axis=0))[0, -10:][0, ::-1]).flatten()\n",
    "[reverse_dic[v] for v in top_10]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IE\n",
      "NS\n",
      "FT\n",
      "JP\n"
     ]
    }
   ],
   "source": [
    "personality_types = ['IE', 'NS', 'FT', 'JP']\n",
    "\n",
    "for l in range(len(personality_types)):\n",
    "    print(personality_types[l])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: 1st posts in tf-idf representation\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.08105478 0.07066064\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.04516864 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.05321691 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.0871647  0.         0.         0.\n",
      " 0.         0.         0.         0.05506308 0.0708757  0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.16585935 0.         0.         0.09676192 0.\n",
      " 0.         0.04970682 0.         0.         0.         0.\n",
      " 0.07397056 0.         0.         0.         0.         0.\n",
      " 0.         0.0748045  0.07639898 0.09185775 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.05133662 0.         0.09442732\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.09657087 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.07062049 0.         0.\n",
      " 0.         0.         0.04405493 0.         0.05892624 0.11838033\n",
      " 0.         0.         0.         0.         0.1245151  0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.15886654 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.08435344 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.04471932\n",
      " 0.         0.         0.         0.07063387 0.         0.\n",
      " 0.29304485 0.         0.         0.         0.         0.\n",
      " 0.         0.18141448 0.         0.         0.         0.\n",
      " 0.         0.12564763 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.05834299 0.         0.         0.08003112\n",
      " 0.08435344 0.         0.         0.09115444 0.         0.08189961\n",
      " 0.         0.13411106 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.05631577 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.05557735 0.         0.\n",
      " 0.         0.         0.         0.         0.0611348  0.09448648\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.07930797 0.09349615 0.         0.06379457 0.         0.17160741\n",
      " 0.         0.         0.         0.13427658 0.         0.\n",
      " 0.08006774 0.         0.         0.         0.         0.\n",
      " 0.         0.06885442 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.09793451 0.         0.\n",
      " 0.         0.         0.05268964 0.         0.05993985 0.05477763\n",
      " 0.         0.04997463 0.         0.         0.         0.\n",
      " 0.         0.         0.09867431 0.         0.         0.\n",
      " 0.09487394 0.         0.15341726 0.         0.         0.\n",
      " 0.         0.         0.08813151 0.07690585 0.07177948 0.\n",
      " 0.         0.09843694 0.         0.         0.         0.\n",
      " 0.         0.05941035 0.08188014 0.         0.         0.\n",
      " 0.         0.         0.09612974 0.06395483 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.06973746 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.12067215\n",
      " 0.         0.         0.         0.         0.         0.09674118\n",
      " 0.         0.0615769  0.         0.         0.         0.\n",
      " 0.         0.         0.         0.07446814 0.         0.\n",
      " 0.         0.         0.         0.08993396 0.         0.\n",
      " 0.         0.         0.         0.         0.09901671 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.05946565 0.         0.         0.         0.05873657 0.\n",
      " 0.         0.         0.         0.09634949 0.         0.04805982\n",
      " 0.         0.08968056 0.         0.         0.0860263  0.\n",
      " 0.         0.         0.         0.06318282 0.         0.\n",
      " 0.         0.04256832 0.         0.         0.         0.\n",
      " 0.06642087 0.         0.         0.         0.09201473 0.\n",
      " 0.         0.0831116  0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.06971149\n",
      " 0.09554125 0.04625983 0.08531558 0.         0.         0.\n",
      " 0.06799661 0.07466644 0.         0.         0.         0.09843694\n",
      " 0.         0.         0.         0.06502346 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.06869092 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.08067849 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.0466968  0.0539756  0.08760887 0.\n",
      " 0.1533845  0.         0.         0.         0.         0.09298479\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.10580777 0.\n",
      " 0.         0.11100899 0.13361762 0.         0.         0.\n",
      " 0.06046932 0.         0.08258902 0.         0.         0.2392193\n",
      " 0.         0.09217882 0.         0.04223906 0.         0.\n",
      " 0.08665848 0.04111178 0.         0.         0.16434695 0.04117693\n",
      " 0.         0.         0.         0.07231244 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.11467609 0.09387096 0.         0.         0.         0.\n",
      " 0.         0.         0.04833236 0.         0.         0.\n",
      " 0.        ]\n"
     ]
    }
   ],
   "source": [
    "print(\"X: 1st posts in tf-idf representation\\n%s\" % X_tfidf[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "# Posts in tf-idf representation\n",
    "X = X_tfidf"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IE Accuracy: 77.86%\n",
      "NS Accuracy: 86.03%\n",
      "FT Accuracy: 67.41%\n",
      "JP Accuracy: 61.58%\n",
      "saved models\n"
     ]
    }
   ],
   "source": [
    "# SVM model for MBTI dataset\n",
    "# Individually training each mbti personlity type\n",
    "# setup parameters for xgboost\n",
    "params = {}\n",
    "params['n_estimators'] = 200  # 100\n",
    "params['max_depth'] = 2  # 3\n",
    "params['nthread'] = 8  # 1\n",
    "params['learning_rate'] = 0.2  # 0.1\n",
    "\n",
    "models = []\n",
    "for l in range(len(personality_types)):\n",
    "    Y = list_personality[:, l]\n",
    "\n",
    "    # split data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=12)\n",
    "\n",
    "    # fit model on training data\n",
    "    model = MultinomialNB() # **params)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # make predictions for test data\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    predictions = [round(value) for value in y_pred]\n",
    "    # evaluate predictions\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "\n",
    "    print(\"%s Accuracy: %.2f%%\" % (personality_types[l], accuracy * 100.0))\n",
    "\n",
    "    models.append(model)\n",
    "\n",
    "with open('model/tfizer.pkl', 'wb') as f:\n",
    "    pkl.dump(tfizer, f)\n",
    "with open('model/cntizer.pkl', 'wb') as f:\n",
    "    pkl.dump(cntizer, f)\n",
    "with open('model/lemmatizer.pkl', 'wb') as f:\n",
    "    pkl.dump(lemmatiser, f)\n",
    "\n",
    "for name, model in zip(personality_types, models):\n",
    "    with open(f'model/{name}.pkl', 'wb') as f:\n",
    "        pkl.dump(model, f)\n",
    "\n",
    "print('saved models')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}